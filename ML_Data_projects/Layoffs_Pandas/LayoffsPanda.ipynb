{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98529777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bba5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/home/dean/git/SQL/layoffs_project/layoffs.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'layoffs.csv' not found. Please make sure the file is in the correct directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59663f30",
   "metadata": {},
   "source": [
    "Data Cleaning//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50543fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Removing Duplicates:\n",
      "Number of rows before removing duplicates: 2361\n",
      "Number of rows after removing duplicates: 2356\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove Duplicates\n",
    "print(\"1. Removing Duplicates:\")\n",
    "print(f\"Number of rows before removing duplicates: {len(df)}\")\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(f\"Number of rows after removing duplicates: {len(df_no_duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba7aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized company names (removed unnecessary spaces).\n"
     ]
    }
   ],
   "source": [
    "# 2. Standardize the data\n",
    "\n",
    "# Standardize company names (remove leading/trailing spaces)\n",
    "df_no_duplicates.loc[:, 'company'] = df_no_duplicates['company'].str.strip()\n",
    "print(\"Standardized company names (removed unnecessary spaces).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6460f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized industry names (grouped crypto variations).\n"
     ]
    }
   ],
   "source": [
    "# Standardize industry (handle 'Crypto currency' and variations)\n",
    "df_no_duplicates.loc[:,'industry'] = df_no_duplicates['industry'].str.replace(r'.*crypto.*', 'Crypto', regex=True)\n",
    "print(\"Standardized industry names (grouped crypto variations).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53cee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized country names (removed trailing periods from 'USA.').\n"
     ]
    }
   ],
   "source": [
    "# Standardize country (remove trailing '.')\n",
    "df_no_duplicates.loc[:,'country'] = df_no_duplicates['country'].str.rstrip('.')\n",
    "print(\"Standardized country names (removed trailing periods from 'USA.').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c3a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 'date' column to datetime objects.\n"
     ]
    }
   ],
   "source": [
    "# Convert 'date' column to datetime objects\n",
    "df_no_duplicates.loc[:,'date'] = pd.to_datetime(df_no_duplicates['date'], format='%m/%d/%Y')\n",
    "print(\"Converted 'date' column to datetime objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76b1b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Deal with Null values, blanks, and removal of unnecessary artifacts\n",
    "\n",
    "# Replace blank strings in 'industry' with NaN\n",
    "df_no_duplicates.loc[:,'industry'] = df_no_duplicates['industry'].replace('', pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49984f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing 'industry' values based on company name where possible.\n"
     ]
    }
   ],
   "source": [
    "# Fill missing 'industry' values based on company name\n",
    "\n",
    "def fill_missing_industry(series):\n",
    "    if pd.isna(series.iloc[0]):\n",
    "        non_na_values = series.dropna().unique()\n",
    "        if len(non_na_values) == 1:\n",
    "            return non_na_values[0]\n",
    "    return series.iloc[0]\n",
    "\n",
    "df_no_duplicates.loc[:,'industry'] = df_no_duplicates.groupby('company')['industry'].transform(fill_missing_industry)\n",
    "print(\"Filled missing 'industry' values based on company name where possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4599bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after handling critical nulls: 1995\n",
      "\n",
      "Cleaned Data (first few rows):\n",
      "     company       location     industry  total_laid_off  percentage_laid_off  \\\n",
      "0  Atlassian         Sydney        Other           500.0                 0.05   \n",
      "1   SiriusXM  New York City        Media           475.0                 0.08   \n",
      "2     Alerzo         Ibadan       Retail           400.0                  NaN   \n",
      "3     UpGrad         Mumbai    Education           120.0                  NaN   \n",
      "4       Loft      Sao Paulo  Real Estate           340.0                 0.15   \n",
      "\n",
      "                  date     stage        country  funds_raised_millions  \n",
      "0  2023-03-06 00:00:00  Post-IPO      Australia                  210.0  \n",
      "1  2023-03-06 00:00:00  Post-IPO  United States                  525.0  \n",
      "2  2023-03-06 00:00:00  Series B        Nigeria                   16.0  \n",
      "3  2023-03-06 00:00:00   Unknown          India                  631.0  \n",
      "4  2023-03-03 00:00:00   Unknown         Brazil                  788.0  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where both 'total_laid_off' and 'percentage_laid_off' are NaN\n",
    "df_cleaned = df_no_duplicates.dropna(subset=['total_laid_off', 'percentage_laid_off'], how='all')\n",
    "print(f\"Number of rows after handling critical nulls: {len(df_cleaned)}\")\n",
    "print(\"\\nCleaned Data (first few rows):\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fc7a8",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d77e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest layoff in a single occurrence: 12000.0\n"
     ]
    }
   ],
   "source": [
    "# What was the largest lay off in a single occurrence?\n",
    "largest_layoff = df_cleaned['total_laid_off'].max()\n",
    "print(f\"Largest layoff in a single occurrence: {largest_layoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f76708a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Companies with the highest percentage laid off:\n",
      "['Kandela', 'DUX Education', 'Locomation', 'Fipola', 'EMX Digital', 'WeTrade', 'Medly', 'Openpay', 'Mode Global', 'Earth Rides', 'Britishvolt', 'Arch Oncology', 'Lantern', 'Wyre', 'YourGrocer', 'Brodmann17', 'Digital Surge', 'Lora DiCarlo', 'Bitfront', 'BlockFi', 'Assure', 'GoodGood', 'GloriFi', 'Kite', 'Deliveroo Australia', 'Protocol', 'Nirvana Money', 'Wavely', 'Faze Medicines', 'Planetly', 'Fifth Season', 'GoNuts', 'Nuri', 'Flux Systems', 'Qin1', 'Playdots', 'Pastel', 'Pesto', 'Kitty Hawk', 'Propzy', 'CommonBond', 'Lido Learning', 'Simple Feast', 'The Wing', 'Reali', 'ShopX', 'Edmodo', 'Pollen', 'Haus', 'Perceptive Automata', 'Metigy', 'Yabonza', 'Soluto', 'Airlift', 'Butler Hospitality', 'WanderJaunt', 'Crejo.Fun', 'Gavelytics', 'Volt Bank', 'Kune', 'SuperLearn', 'SummerBio', 'JetClosing', 'The Grommet', 'Kaodim', 'Udayy', 'BeyondMinds', 'Subspace', 'SEND', 'Halcyon Health', 'Ahead', 'Fast', 'Ozy Media', 'Katerra', 'Madefire', 'Limelight', 'Hubba', 'Pocketmath', 'Aura Financial', 'Simple', 'Bridge Connector', 'Rubica', 'Quibi', 'HubHaus', 'Awok', 'Lumina Networks', 'Eatsy', 'Buy.com / Rakuten', 'Sorabel', 'Engine eCommerce', 'Dark', 'Stockwell AI', 'Bluprint', 'Stay Alfred', 'Dotscience', 'Masse', 'Deliv', 'WeFit', 'Jump', 'Airy Rooms', 'Automatic', 'PicoBrew', 'Stoqo', 'TutorMundi', 'Motif Investing', 'Purse', 'Atsu', 'The Modist', 'Amplero', 'HOOQ', 'Consider.co', 'Service', 'Ejento', 'Popin', 'Help.com', 'Service']\n"
     ]
    }
   ],
   "source": [
    "# Who had the most people laid off as a percentage of the company?\n",
    "most_laid_off_percent = df_cleaned['percentage_laid_off'].max()\n",
    "print(\"\\nCompanies with the highest percentage laid off:\")\n",
    "print(df_cleaned[df_cleaned['percentage_laid_off'] == most_laid_off_percent]['company'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3124b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Company with the largest total layoffs:\n",
      "company\n",
      "Amazon            18150.0\n",
      "Google            12000.0\n",
      "Meta              11000.0\n",
      "Salesforce        10090.0\n",
      "Microsoft         10000.0\n",
      "                   ...   \n",
      "Reach                 0.0\n",
      "DriveWealth           0.0\n",
      "Locomation            0.0\n",
      "Dude Solutions        0.0\n",
      "PagSeguro             0.0\n",
      "Name: total_laid_off, Length: 1633, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What company had the largest lay off in total?\n",
    "total_layoffs_by_company = df_cleaned.groupby('company')['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nCompany with the largest total layoffs:\")\n",
    "print(total_layoffs_by_company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3f3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time frame of the data: 2020-03-11 to 2023-03-06\n"
     ]
    }
   ],
   "source": [
    "# What is the time frame of the data we got?\n",
    "datetime_dates = df_cleaned['date'][pd.to_datetime(df_cleaned['date'], errors='coerce').notna()]\n",
    "min_date = datetime_dates.min()\n",
    "max_date = datetime_dates.max()\n",
    "print(f\"\\nTime frame of the data: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3597b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Industry with the most layoffs:\n",
      "industry\n",
      "Consumer    45541.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Which industry had the most laying off?\n",
    "total_layoffs_by_industry = df_cleaned.groupby('industry')['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nIndustry with the most layoffs:\")\n",
    "print(total_layoffs_by_industry.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5bc376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country with the most layoffs:\n",
      "country\n",
      "United States    256559.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Which country had the most laying off?\n",
    "total_layoffs_by_country = df_cleaned.groupby('country')['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nCountry with the most layoffs:\")\n",
    "print(total_layoffs_by_country.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year with the most layoffs:\n",
      "date\n",
      "2022.0    160661.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What year had the most layoffs\n",
    "total_layoffs_by_year = df_cleaned.dropna(subset=['date']).groupby(df_cleaned['date'].dt.year)['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nYear with the most layoffs:\")\n",
    "print(total_layoffs_by_year.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8facdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage with the most layoffs:\n",
      "stage\n",
      "Post-IPO    204132.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What stage had the most layoffs?\n",
    "total_layoffs_by_stage = df_cleaned.groupby('stage')['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nStage with the most layoffs:\")\n",
    "print(total_layoffs_by_stage.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "537c21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total layoffs by month:\n",
      "month_year\n",
      "2023-01    84714.0\n",
      "2022-11    53451.0\n",
      "2023-02    36493.0\n",
      "2020-04    26710.0\n",
      "2020-05    25804.0\n",
      "2022-10    17406.0\n",
      "2022-06    17394.0\n",
      "2022-07    16223.0\n",
      "2022-08    13055.0\n",
      "2022-05    12885.0\n",
      "2022-12    10329.0\n",
      "2020-03     9628.0\n",
      "2020-06     7627.0\n",
      "2020-07     7112.0\n",
      "2021-01     6813.0\n",
      "2022-09     5881.0\n",
      "2022-03     5714.0\n",
      "2023-03     4470.0\n",
      "2022-04     4128.0\n",
      "2022-02     3685.0\n",
      "2021-06     2434.0\n",
      "2021-11     2070.0\n",
      "2020-08     1969.0\n",
      "2021-08     1867.0\n",
      "2021-12     1200.0\n",
      "2021-02      868.0\n",
      "2020-12      852.0\n",
      "2020-09      609.0\n",
      "2022-01      510.0\n",
      "2020-10      450.0\n",
      "2021-04      261.0\n",
      "2020-11      237.0\n",
      "2021-09      161.0\n",
      "2021-07       80.0\n",
      "2021-03       47.0\n",
      "2021-10       22.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11363/1418196045.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['month_year'] = df_cleaned['date'].dt.strftime('%Y-%m')\n"
     ]
    }
   ],
   "source": [
    "# How many layoffs happened? show by months\n",
    "df_cleaned['month_year'] = df_cleaned['date'].dt.strftime('%Y-%m')\n",
    "total_layoffs_by_month = df_cleaned.groupby('month_year')['total_laid_off'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTotal layoffs by month:\")\n",
    "print(total_layoffs_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c6c8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rolling total of layoffs by month:\n",
      "month_year\n",
      "2020-03      9628.0\n",
      "2020-04     36338.0\n",
      "2020-05     62142.0\n",
      "2020-06     69769.0\n",
      "2020-07     76881.0\n",
      "2020-08     78850.0\n",
      "2020-09     79459.0\n",
      "2020-10     79909.0\n",
      "2020-11     80146.0\n",
      "2020-12     80998.0\n",
      "2021-01     87811.0\n",
      "2021-02     88679.0\n",
      "2021-03     88726.0\n",
      "2021-04     88987.0\n",
      "2021-06     91421.0\n",
      "2021-07     91501.0\n",
      "2021-08     93368.0\n",
      "2021-09     93529.0\n",
      "2021-10     93551.0\n",
      "2021-11     95621.0\n",
      "2021-12     96821.0\n",
      "2022-01     97331.0\n",
      "2022-02    101016.0\n",
      "2022-03    106730.0\n",
      "2022-04    110858.0\n",
      "2022-05    123743.0\n",
      "2022-06    141137.0\n",
      "2022-07    157360.0\n",
      "2022-08    170415.0\n",
      "2022-09    176296.0\n",
      "2022-10    193702.0\n",
      "2022-11    247153.0\n",
      "2022-12    257482.0\n",
      "2023-01    342196.0\n",
      "2023-02    378689.0\n",
      "2023-03    383159.0\n",
      "Name: total_laid_off, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show progress of layoffs with each month passing and accumulate the sum\n",
    "rolling_layoffs = df_cleaned.groupby('month_year')['total_laid_off'].sum().cumsum()\n",
    "print(\"\\nRolling total of layoffs by month:\")\n",
    "print(rolling_layoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6290e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 companies with the highest number of layoffs for each year:\n",
      "          company    year  total_laid_off  rank\n",
      "1631         Uber  2020.0          7525.0   1.0\n",
      "194   Booking.com  2020.0          4375.0   2.0\n",
      "653       Groupon  2020.0          2800.0   3.0\n",
      "1503       Swiggy  2020.0          2250.0   4.0\n",
      "38         Airbnb  2020.0          1900.0   5.0\n",
      "247     Bytedance  2021.0          3600.0   1.0\n",
      "809       Katerra  2021.0          2434.0   2.0\n",
      "1799       Zillow  2021.0          2000.0   3.0\n",
      "757     Instacart  2021.0          1877.0   4.0\n",
      "1746  WhiteHat Jr  2021.0          1800.0   5.0\n",
      "979          Meta  2022.0         11000.0   1.0\n",
      "56         Amazon  2022.0         10150.0   2.0\n",
      "315         Cisco  2022.0          4100.0   3.0\n",
      "1167      Peloton  2022.0          4084.0   4.0\n",
      "283       Carvana  2022.0          4000.0   5.0\n",
      "1181      Philips  2022.0          4000.0   5.0\n",
      "643        Google  2023.0         12000.0   1.0\n",
      "983     Microsoft  2023.0         10000.0   2.0\n",
      "511      Ericsson  2023.0          8500.0   3.0\n",
      "57         Amazon  2023.0          8000.0   4.0\n",
      "1338   Salesforce  2023.0          8000.0   4.0\n",
      "433          Dell  2023.0          6650.0   5.0\n"
     ]
    }
   ],
   "source": [
    "# Identify the top 5 companies with the highest number of layoffs for each year\n",
    "layoffs_by_company_year = df_cleaned.groupby(['company', df_cleaned['date'].dt.year])['total_laid_off'].sum().reset_index()\n",
    "layoffs_by_company_year = layoffs_by_company_year.rename(columns={'date': 'year'})\n",
    "layoffs_by_company_year['rank'] = layoffs_by_company_year.groupby('year')['total_laid_off'].rank(method='dense', ascending=False)\n",
    "top_5_companies_by_year = layoffs_by_company_year[layoffs_by_company_year['rank'] <= 5]\n",
    "print(\"\\nTop 5 companies with the highest number of layoffs for each year:\")\n",
    "print(top_5_companies_by_year.sort_values(by=['year', 'rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794642fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
